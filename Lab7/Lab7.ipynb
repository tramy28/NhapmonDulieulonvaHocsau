{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAB 7: PHÂN TÍCH DỮ LIỆU DẠNG VĂN BẢN VỚI NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Thao tác với thư viện NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trực tiếp tải xuống gói yêu cầu\n",
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gutenberg files :  ['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "# Xem tên các tập tin có trong gói 'gutenberg'\n",
    "gb = nltk.corpus.gutenberg\n",
    "    #sử dụng module corpus.gutenberg để truy cập vào dữ liệu trong gói 'gutenberg'.\n",
    "print(\"Gutenberg files : \", gb.fileids())\n",
    "    # fileids(): lấy danh sách các tập tin có sẵn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tải văn bản của tác phẩm \"Macbeth\" từ gói dữ liệu 'gutenberg' trong NLTK và lưu trữ nó vào biến macbeth\n",
    "macbeth = nltk.corpus.gutenberg.words('shakespeare-macbeth.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23140"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chiều dài của văn bản - số từ\n",
    "len(macbeth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[',\n",
       " 'The',\n",
       " 'Tragedie',\n",
       " 'of',\n",
       " 'Macbeth',\n",
       " 'by',\n",
       " 'William',\n",
       " 'Shakespeare',\n",
       " '1603',\n",
       " ']']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hiển thị 10 từ đầu tiên của tập tin\n",
    "macbeth [:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tìm một từ với NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 3 of 3 matches:\n",
      "nts with Dishes and Seruice ouer the Stage . Then enter Macbeth Macb . If it we\n",
      "with mans Act , Threatens his bloody Stage : byth ' Clock ' tis Day , And yet d\n",
      " struts and frets his houre vpon the Stage , And then is heard no more . It is \n"
     ]
    }
   ],
   "source": [
    "# Tìm từ 'Stage' xuất hiện trong văn bản text\n",
    "text = nltk.Text(macbeth) # Chuyển đổi văn bản sang đối tượng Text\n",
    "text.concordance('Stage') # Tìm kiếm từ 'Stage' và in ra các vị trí xuất hiện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the_. bloody_: the_,\n"
     ]
    }
   ],
   "source": [
    "# Tìm từ xuất hiện trước từ 'Stage'\n",
    "text.common_contexts(['Stage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day time face warre ayre king bleeding man reuolt serieant like\n",
      "knowledge broyle shew head spring heeles hare thane skie\n"
     ]
    }
   ],
   "source": [
    "# Tìm từ tương tự từ ‘Stage’\n",
    "text.similar('Stage')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Phân tích tần số của các từ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 1962),\n",
       " ('.', 1235),\n",
       " (\"'\", 637),\n",
       " ('the', 531),\n",
       " (':', 477),\n",
       " ('and', 376),\n",
       " ('I', 333),\n",
       " ('of', 315),\n",
       " ('to', 311),\n",
       " ('?', 241)]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Xem 10 từ thông dụng nhất trong văn bản xuất hiện bao nhiêu lần\n",
    "fd = nltk.FreqDist(macbeth) # Sử dụng FreqDist để đếm tần số xuất hiện của từng từ\n",
    "fd.most_common(10) # Lấy 10 từ thông dụng nhất và số lần xuất hiện của chúng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download stopword\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ma',\n",
       " 'once',\n",
       " 'yourselves',\n",
       " \"mustn't\",\n",
       " 'will',\n",
       " 'isn',\n",
       " 'other',\n",
       " 'those',\n",
       " 'few',\n",
       " 'against']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Xem các stopword trong tiếng Anh\n",
    "sw = set(nltk.corpus.stopwords.words('english')) # Lấy danh sách stopword trong tiếng Anh từ NLTK\n",
    "print(len(sw)) # In ra số lượng stopword\n",
    "list(sw)[:10] # In ra 10 stopword đầu tiên"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14946"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loại bỏ các từ stopword trong biến macbeth\n",
    "macbeth_filtered = [w for w in macbeth if w.lower() not in sw] # Lấy văn bản của Macbeth từ gutenberg\n",
    "len(macbeth_filtered) # Loại bỏ các từ stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 1962),\n",
       " ('.', 1235),\n",
       " (\"'\", 637),\n",
       " (':', 477),\n",
       " ('?', 241),\n",
       " ('Macb', 137),\n",
       " ('haue', 117),\n",
       " ('-', 100),\n",
       " ('Enter', 80),\n",
       " ('thou', 63)]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trả về 10 từ phổ biến, các từ stopword đã được loại bỏ\n",
    "fd = nltk.FreqDist(macbeth_filtered)\n",
    "fd.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('macb', 137),\n",
       " ('haue', 122),\n",
       " ('thou', 90),\n",
       " ('enter', 81),\n",
       " ('shall', 68),\n",
       " ('macbeth', 62),\n",
       " ('vpon', 62),\n",
       " ('thee', 61),\n",
       " ('macd', 58),\n",
       " ('vs', 57)]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loại bỏ các dấu câu\n",
    "import string\n",
    "punctuation = set(string.punctuation)\n",
    "macbeth_filtered2 = [w.lower() for w in macbeth if w.lower() not in sw and w.lower() not in punctuation]\n",
    "fd = nltk.FreqDist(macbeth_filtered2)\n",
    "fd.most_common(10) \n",
    "    #Kết quả ra 10 từ phổ biến không phải là dấu câu và số lần xuất hiện"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Lựa chọn các từ trong văn bản"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Assassination',\n",
       " 'Chamberlaines',\n",
       " 'Distinguishes',\n",
       " 'Gallowgrosses',\n",
       " 'Metaphysicall',\n",
       " 'Northumberland',\n",
       " 'Voluptuousnesse',\n",
       " 'commendations',\n",
       " 'multitudinous',\n",
       " 'supernaturall',\n",
       " 'vnaccompanied']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rút trích các từ có độ dài lớn nhất, ví dụ các từ có độ dài lớn hơn 12 ký tự\n",
    "long_words = [w for w in macbeth if len(w)> 12]\n",
    "sorted(long_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Auaricious',\n",
       " 'Gracious',\n",
       " 'Industrious',\n",
       " 'Iudicious',\n",
       " 'Luxurious',\n",
       " 'Malicious',\n",
       " 'Obliuious',\n",
       " 'Pious',\n",
       " 'Rebellious',\n",
       " 'compunctious',\n",
       " 'furious',\n",
       " 'gracious',\n",
       " 'pernicious',\n",
       " 'pernitious',\n",
       " 'pious',\n",
       " 'precious',\n",
       " 'rebellious',\n",
       " 'sacrilegious',\n",
       " 'serious',\n",
       " 'spacious',\n",
       " 'tedious']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rút trích các từ có chứa chuỗi ‘ious’\n",
    "ious_words = [w for w in macbeth if 'ious' in w]\n",
    "ious_words = set(ious_words)\n",
    "sorted(ious_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Bigrams và collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('enter', 'macbeth'), 16),\n",
       " (('exeunt', 'scena'), 15),\n",
       " (('thane', 'cawdor'), 13),\n",
       " (('knock', 'knock'), 10),\n",
       " (('st', 'thou'), 9),\n",
       " (('thou', 'art'), 9),\n",
       " (('lord', 'macb'), 9),\n",
       " (('haue', 'done'), 8),\n",
       " (('macb', 'haue'), 8),\n",
       " (('good', 'lord'), 8),\n",
       " (('let', 'vs'), 7),\n",
       " (('enter', 'lady'), 7),\n",
       " (('wee', 'l'), 7),\n",
       " (('would', 'st'), 6),\n",
       " (('macbeth', 'macb'), 6)]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lọc các bigram sau khi đã loại các stopword và các dấu câu\n",
    "bgrms = nltk.FreqDist(nltk.bigrams(macbeth_filtered2))\n",
    "bgrms.most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('knock', 'knock', 'knock'), 6),\n",
       " (('enter', 'macbeth', 'macb'), 5),\n",
       " (('enter', 'three', 'witches'), 4),\n",
       " (('exeunt', 'scena', 'secunda'), 4),\n",
       " (('good', 'lord', 'macb'), 4),\n",
       " (('three', 'witches', '1'), 3),\n",
       " (('exeunt', 'scena', 'tertia'), 3),\n",
       " (('thunder', 'enter', 'three'), 3),\n",
       " (('exeunt', 'scena', 'quarta'), 3),\n",
       " (('scena', 'prima', 'enter'), 3)]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgrms = nltk.FreqDist(nltk.trigrams (macbeth_filtered2)) # nltk.trigrams: tạo ra các cặp ba từ liên tiếp trong văn bản\n",
    "tgrms.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Sử dụng văn bản trên mạng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeffThe Project Gutenberg eBook of Crime and Punishment, by Fyodor Dostoevsky\\r'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import thư viện và mở url để đọc file\n",
    "from urllib import request\n",
    "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
    "response = request.urlopen(url)\n",
    "raw = response.read().decode('utf8')\n",
    "raw[:75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Project Gutenberg eBook of Crime and Punishment, by Fyodor Dostoevsky\\r\\n'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# File văn bản từ URL có BOM, việc sử dụng 'utf-8-sig' sẽ loại bỏ BOM trước khi giải mã\n",
    "from urllib import request\n",
    "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
    "response = request.urlopen(url)\n",
    "raw = response.read().decode('utf-8-sig')\n",
    "raw[:75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Project',\n",
       " 'Gutenberg',\n",
       " 'eBook',\n",
       " 'of',\n",
       " 'Crime',\n",
       " 'and',\n",
       " 'Punishment',\n",
       " ',',\n",
       " 'by',\n",
       " 'Fyodor',\n",
       " 'Dostoevsky']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sử dụng thư viện NLTK để tokenize (word_tokenize) nội dung văn bản từ URL, sau đó tạo một đối tượng Text\n",
    "tokens = nltk.word_tokenize (raw)\n",
    "webtext = nltk.Text (tokens)\n",
    "webtext[:12] # Hiển thị 12 từ đầu tiên"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Rút trích văn bản từ trang html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html>\\n<html class=\"client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rút trích văn bản từ trang html.\n",
    "url = \"https://vi.wikipedia.org/wiki/Vi%E1%BB%87t_Nam\"\n",
    "html = request.urlopen(url).read().decode('utf8')\n",
    "html[:120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dùng thư viện bs4 cung cấp các trình phân tích cú pháp phù hợp có thể nhận dạng HTML và trích xuất văn bản\n",
    "from bs4 import BeautifulSoup\n",
    "raw = BeautifulSoup(html, \"html.parser\").get_text()\n",
    "tokens = nltk.word_tokenize(raw)\n",
    "text = nltk.Text(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Phân tích cảm xúc người dùng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download các movie review\n",
    "nltk.download('movie_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xây dựng tập train dựa vào copus trên\n",
    "import random\n",
    "reviews = nltk.corpus.movie_reviews\n",
    "documents = [(list(reviews.words(fileid)), category)\n",
    "             for category in reviews.categories()\n",
    "            for fileid in reviews.fileids(category)]\n",
    "random.shuffle(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the seasoned capt . dudley smith ( james cromwell ) questions his new protege , ed exley ( guy pearce ) about his political views when it comes to police work . \" would you plant evidence to get a conviction if you knew the person to be guilty ? would you shoot a man in the back if you knew that it was the only way to make sure he got what he deserved ? \" exley , squeaky clean , innocent , and pent with the desire to perform his new duties with integrity and honesty quickly answers with a pointed ' no . ' stunned , smith pleads , \" then , for the love of god , don ' t be a detective . \" la confidential is a dandy piece of filmmaking that brings us back to the classic times of hollywood in the 50s . organized crime was the biggest concern , corruption existed everywhere , and every cop had his own way of getting the job done . there was \" hollywood \" jack vincennes ( kevin spacey ) who gets most of his information from the editor of a sleazy tell - all / show - all tabloid , sid hudgeons ( danny devito ) . sporting dark sunglasses and a white sports coat , he isn ' t afraid to flash his tinseltown style . detective bud white ( russell crowe ) has only a narrow - minded view of the law , and he uses excessive force to exact his unique kind of interrogation and brutal justice . and then there is golden boy exley , wide - eyed and ambitious but extremely naive . a horrible multiple murder begins to draw these three men together , each of whom become inexorably connected in their search for truth , action , and personal vindication . almost immediately , an arrest is made . but something doesn ' t seem quite right . new developments become sinuous as the story begins to draw us deeper into the labyrinth of corruption and crime within the department . suspicion and questions begin to mount . odd alliances are created . and as layers of the truth become revealed , the story grows more complex and intriguing by the moment . when the mystery expands , we see other seemingly guilty characters enter the lives of these three cops , including a high - priced hooker ( kim basinger ) and a shady millionaire ( david strathairn ) . all of the characters ' levels of involvement are not immediately clear . we watch with riveted fascination to learn more about why they are there and their stake in the case . we sense a cross - pollination of clues that brings us , the audience , closer to the truth . we hope that the three detectives can throw away their spite for one another and can pull their resources together in order to untangle the intricately created web of mystery . absorbing and affecting , this movie has all the necessary ingredients to create a terrific gourmet serving of film noir . it boasts mysterious figures in control , double - crosses , hard information from sleazy sources , dirty politicians , corrupt cops , haunting women , and hard - nosed cops experiencing moral ambiguity . the underlying mystery is compelling . the look and feel of the movie is gorgeous . the entire cast is crisp and wonderful to watch ( i wouldn ' t be surprised if cromwell received an oscar nomination for his performance ) . and , the dialogue is rich and memorable . the best line of the movie is spoken when a key player dispenses advice to one of the detectives unsure of what to do . \" don ' t start trying to do the right thing , boy - o , \" says he . \" you haven ' t had the practice . \" \" l . a . confidential '' triumphantly achieves the rare gift of complexity and coherence while satisfying our desire for a good old - fashioned movie .\n"
     ]
    }
   ],
   "source": [
    "# Xem nội dung review đầu tiên (dòng 0, cột 0)\n",
    "first_review = ' '.join(documents[0][0])\n",
    "print(first_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pos'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Xem nội dung review đầu tiên (dòng 0, cột 1)\n",
    "documents[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo bảng phân phối tần số các từ trong copus, cần chuyển sang dạng list,\n",
    "all_words = nltk.FreqDist(w.lower() for w in reviews.words())\n",
    "word_features = list(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xác định một hàm để tính toán các đặc trưng\n",
    "def document_features(document, word_features):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['{}'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Định nghĩa hàm document_features(), bạn tạo 1 tập các documents\n",
    "featuresets = [(document_features(d,word_features), c) for (d,c) in documents]\n",
    "len(featuresets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo tập train và tập test: 1500 dòng đầu dùng cho tập train và \n",
    "# 500 dòng còn lại dùng cho tập test để đánh giá độ chính xác của mô hình\n",
    "train_set, test_set = featuresets[1500:], featuresets[:500]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.776\n"
     ]
    }
   ],
   "source": [
    "#Dùng thuật toán Naïve Bayes để phân loại, dùng thư viện NLTK. \n",
    "# Sau đó tính toán độ chính xác của thuật toán\n",
    "train_set, est_set = featuresets[1500:], featuresets[:500]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             wonderfully = True              pos : neg    =     12.4 : 1.0\n",
      "                 carries = True              pos : neg    =      9.7 : 1.0\n",
      "               pointless = True              neg : pos    =      9.6 : 1.0\n",
      "                   waste = True              neg : pos    =      8.9 : 1.0\n",
      "                 decades = True              pos : neg    =      8.4 : 1.0\n",
      "             exceptional = True              pos : neg    =      8.4 : 1.0\n",
      "                 surreal = True              pos : neg    =      8.4 : 1.0\n",
      "                   awful = True              neg : pos    =      8.1 : 1.0\n",
      "                  period = True              pos : neg    =      7.9 : 1.0\n",
      "              remembered = True              pos : neg    =      7.7 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Bài tập áp dụng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Viết chương trình Python với thư viện NLTK để liệt kê các tên của copus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'book'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection book\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NLTK corpus readers.  The modules in this package provide functions\n",
      "that can be used to read corpus files in a variety of formats.  These\n",
      "functions can be used to read both the corpus files that are\n",
      "distributed in the NLTK corpus package, and corpus files that are part\n",
      "of external corpora.\n",
      "\n",
      "Available Corpora\n",
      "=================\n",
      "\n",
      "Please see https://www.nltk.org/nltk_data/ for a complete list.\n",
      "Install corpora using nltk.download().\n",
      "\n",
      "Corpus Reader Functions\n",
      "=======================\n",
      "Each corpus module defines one or more \"corpus reader functions\",\n",
      "which can be used to read documents from that corpus.  These functions\n",
      "take an argument, ``item``, which is used to indicate which document\n",
      "should be read from the corpus:\n",
      "\n",
      "- If ``item`` is one of the unique identifiers listed in the corpus\n",
      "  module's ``items`` variable, then the corresponding document will\n",
      "  be loaded from the NLTK corpus package.\n",
      "- If ``item`` is a filename, then that file will be read.\n",
      "\n",
      "Additionally, corpus reader functions can be given lists of item\n",
      "names; in which case, they will return a concatenation of the\n",
      "corresponding documents.\n",
      "\n",
      "Corpus reader functions are named based on the type of information\n",
      "they return.  Some common examples, and their return types, are:\n",
      "\n",
      "- words(): list of str\n",
      "- sents(): list of (list of str)\n",
      "- paras(): list of (list of (list of str))\n",
      "- tagged_words(): list of (str,str) tuple\n",
      "- tagged_sents(): list of (list of (str,str))\n",
      "- tagged_paras(): list of (list of (list of (str,str)))\n",
      "- chunked_sents(): list of (Tree w/ (str,str) leaves)\n",
      "- parsed_sents(): list of (Tree with str leaves)\n",
      "- parsed_paras(): list of (list of (Tree with str leaves))\n",
      "- xml(): A single xml ElementTree\n",
      "- raw(): unprocessed corpus contents\n",
      "\n",
      "For example, to read a list of the words in the Brown Corpus, use\n",
      "``nltk.corpus.brown.words()``:\n",
      "\n",
      "    >>> from nltk.corpus import brown\n",
      "    >>> print(\", \".join(brown.words())) # doctest: +ELLIPSIS\n",
      "    The, Fulton, County, Grand, Jury, said, ...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Tải các tập dữ liệu mặc định của NLTK\n",
    "nltk.download('book')\n",
    "\n",
    "# Liệt kê các tên của các corpus\n",
    "print(nltk.corpus.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Viết chương trình Python với thư viện NLTK để liệt kê danh sách các stopword bằng các ngôn ngữ khác nhau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Danh sách stopwords trong ngôn ngữ english:\n",
      "179\n",
      "**Danh sách stopwords trong ngôn ngữ french:\n",
      "157\n",
      "**Danh sách stopwords trong ngôn ngữ german:\n",
      "232\n",
      "**Danh sách stopwords trong ngôn ngữ spanish:\n",
      "313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Tải các tập dữ liệu mặc định của NLTK (nếu chưa được tải)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Liệt kê danh sách stopword cho các ngôn ngữ khác nhau\n",
    "languages = ['english', 'french', 'german', 'spanish']\n",
    "\n",
    "for lang in languages:\n",
    "    print(f\"**Danh sách stopwords trong ngôn ngữ {lang}:\")\n",
    "    sw = set(nltk.corpus.stopwords.words(lang))\n",
    "    print(len(sw)) \n",
    "    list(sw)[:10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Viết chương trình Python với thư viện NLTK để kiểm tra danh sách các stopword bằng các ngôn ngữ khác nhau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Kiểm tra các từ trong danh sách stopword của ngôn ngữ english:\n",
      "- 'the' là stopword trong ngôn ngữ english\n",
      "- 'et' không phải là stopword trong ngôn ngữ english\n",
      "- 'und' không phải là stopword trong ngôn ngữ english\n",
      "- 'y' là stopword trong ngôn ngữ english\n",
      "\n",
      "**Kiểm tra các từ trong danh sách stopword của ngôn ngữ french:\n",
      "- 'the' không phải là stopword trong ngôn ngữ french\n",
      "- 'et' là stopword trong ngôn ngữ french\n",
      "- 'und' không phải là stopword trong ngôn ngữ french\n",
      "- 'y' là stopword trong ngôn ngữ french\n",
      "\n",
      "**Kiểm tra các từ trong danh sách stopword của ngôn ngữ german:\n",
      "- 'the' không phải là stopword trong ngôn ngữ german\n",
      "- 'et' không phải là stopword trong ngôn ngữ german\n",
      "- 'und' là stopword trong ngôn ngữ german\n",
      "- 'y' không phải là stopword trong ngôn ngữ german\n",
      "\n",
      "**Kiểm tra các từ trong danh sách stopword của ngôn ngữ spanish:\n",
      "- 'the' không phải là stopword trong ngôn ngữ spanish\n",
      "- 'et' không phải là stopword trong ngôn ngữ spanish\n",
      "- 'und' không phải là stopword trong ngôn ngữ spanish\n",
      "- 'y' là stopword trong ngôn ngữ spanish\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Tải các tập dữ liệu mặc định của NLTK (nếu chưa được tải)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Kiểm tra danh sách stopword cho các ngôn ngữ khác nhau\n",
    "languages = ['english', 'french', 'german', 'spanish']\n",
    "words_to_check = ['the', 'et', 'und', 'y']\n",
    "\n",
    "for lang in languages:\n",
    "    print(f\"**Kiểm tra các từ trong danh sách stopword của ngôn ngữ {lang}:\")\n",
    "    sw = set(nltk.corpus.stopwords.words(lang))\n",
    "    for word in words_to_check:\n",
    "        if word.lower() in sw:\n",
    "            print(f\"- '{word}' là stopword trong ngôn ngữ {lang}\")\n",
    "        else:\n",
    "            print(f\"- '{word}' không phải là stopword trong ngôn ngữ {lang}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Viết chương trình Python với thư viện NLTK để loại bỏ các stopword từ một văn bản đã cho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Văn bản sau khi loại bỏ các stop words:\n",
      "sample sentence , showing stop words filtration .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tải tập dữ liệu stopwords cho ngôn ngữ English từ NLTK (nếu chưa được tải)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Văn bản mẫu để loại bỏ stopwords\n",
    "sample_text = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "\n",
    "# Tách từ trong văn bản\n",
    "tokens = word_tokenize(sample_text)\n",
    "\n",
    "# Loại bỏ stopwords cho ngôn ngữ English\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "filtered_text = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "# Kết quả sau khi loại bỏ stopwords\n",
    "filtered_text = ' '.join(filtered_text)\n",
    "print(\"Văn bản sau khi loại bỏ các stop words:\")\n",
    "print(filtered_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
